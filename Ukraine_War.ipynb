{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing Necessary Libraries\n",
    "\n",
    "In this initial step, we import the essential Python libraries required for our project. Each library serves a specific purpose, enabling us to work with Reddit's data and perform data manipulation tasks.\n",
    "\n",
    "- **Pandas (pd)**: We use the `pandas` library to manipulate and analyze data. It's particularly well-suited for handling structured data in a tabular format, which aligns with our data analysis needs.\n",
    "\n",
    "- **PRAW (Python Reddit API Wrapper)**: For accessing Reddit's API, we utilize the `praw` library. It provides a convenient interface for interacting with Reddit, allowing us to retrieve posts, comments, and perform various actions on the platform.\n",
    "\n",
    "- **Time**: The `time` module is included to enable us to introduce controlled time delays within our code. These delays, implemented using `time.sleep()`, are essential for managing the rate at which we make requests to Reddit's API. This helps avoid overloading the API and encountering rate-limiting issues.\n",
    "\n",
    "- **PRAW Exceptions**: The `praw.exceptions` module is imported to handle specific exceptions that might arise during our interactions with Reddit's API. These exceptions include rate limit errors, connection issues, and other potential problems that could occur when retrieving data from Reddit.\n",
    "\n",
    "In summary, this initial step equips us with the necessary tools to interact with Reddit's API, fetch data, and effectively process it for analysis. We're now prepared to use these libraries in subsequent steps to retrieve and analyze data from Reddit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import time\n",
    "import praw.exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Authenticating with the Reddit API\n",
    "\n",
    "To access Reddit's data programmatically, we need to authenticate our Python application using the `praw` library. Here's what each part of the code does:\n",
    "\n",
    "- `import praw`: We import the `praw` library, which is a Python wrapper for the Reddit API. This library simplifies the process of interacting with Reddit's data.\n",
    "\n",
    "- `reddit = praw.Reddit(...)`: We create a Reddit API client object named `reddit`. This object is used for making authenticated requests to the Reddit API. The constructor takes the following parameters:\n",
    "\n",
    "   - `client_id`: This should be replaced with the unique identifier of your Reddit Developer Application, which you obtained when you created the application on the Reddit Developer Portal. It's used to identify your application when making API requests.\n",
    "\n",
    "   - `client_secret`: This key should be replaced with the secret key provided during the creation of your Reddit Developer Application. It's a secret key that, when combined with the client ID, allows your application to securely authenticate with the Reddit API.\n",
    "\n",
    "   - `user_agent`: The user agent is a string that identifies your application and its purpose. It's important to provide a user agent that follows Reddit's guidelines, typically including the name of your application and a version number. For personal projects, you can include your Reddit username or any other descriptive information.\n",
    "\n",
    "With this authenticated `reddit` object, we can now access various Reddit data and perform operations like fetching posts, comments, and more, which will be an essential part of our project.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id='DAuF7LHCr_OM-_PGf-UBaw',\n",
    "    client_secret='uor61HS6MgW5yHYBk8LcOmTlW5j5xQ',\n",
    "    user_agent='Dry_Try8800',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Retrieval and Data Processing\n",
    "\n",
    "In this step, we utilize the Python Reddit API Wrapper (PRAW) library to collect data from Reddit and subsequently process it to create a structured DataFrame for in-depth analysis. Before executing the code, it's essential to ensure that the necessary libraries are installed and Reddit API credentials are correctly configured.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **Subreddit**: By default, the code extracts posts from the 'all' subreddit. However, this setting can be adjusted to target a specific subreddit of your choice.\n",
    "\n",
    "- **Total Posts to Retrieve**: The variable `total_posts_to_retrieve` is initially set to 10,000 in this example. You can customize it to suit your specific data collection requirements.\n",
    "\n",
    "- **Time Filter**: The code applies a time filter to retrieve posts within a specified time frame, such as 'year.' You can modify the `time_filter` variable to focus on a different time period.\n",
    "\n",
    "- **Batch Size**: The `batch_size` variable dictates the number of posts fetched in each batch. This batch-oriented approach is crucial for adhering to Reddit's API rate limits and managing data retrieval effectively.\n",
    "\n",
    "### Data Retrieval Loop\n",
    "\n",
    "The data retrieval process unfolds in a sequence of steps:\n",
    "\n",
    "1. Calculate the number of remaining posts to retrieve in the current batch.\n",
    "\n",
    "2. Implement a 2-second delay between API requests to circumvent potential rate limiting issues.\n",
    "\n",
    "3. Issue an API request to search for posts, utilizing the 'before' parameter for pagination purposes.\n",
    "\n",
    "4. Perform a check to determine if there are additional posts to retrieve. If the query returns no more results, the loop concludes.\n",
    "\n",
    "5. Append the retrieved posts to the `all_posts` list, continuously building the dataset.\n",
    "\n",
    "6. Keep track of counts to monitor the number of posts retrieved and the current batch being processed.\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "After obtaining the data, we engage in a data processing phase to structure it into a Pandas DataFrame for subsequent analysis:\n",
    "\n",
    "1. Initialize an empty list `post_data` to accumulate essential information about each post.\n",
    "\n",
    "2. Iterate through each post:\n",
    "   - Extract data points such as the author's username, post title, URL, the number of upvotes, and more.\n",
    "   - Incorporate a robust approach to handle cases where author information might not be available, guarding against potential `AttributeError` exceptions.\n",
    "   - Appends the extracted data as a dictionary to the `post_data` list.\n",
    "\n",
    "3. Create a DataFrame (`df`) using the `post_data` list. Within this DataFrame, each row corresponds to a Reddit post, and each column represents a distinct attribute.\n",
    "\n",
    "### Data Analysis\n",
    "\n",
    "With the DataFrame (`df`) prepared, a multitude of analyses and insights can be derived from the Reddit data:\n",
    "\n",
    "- Explore the distribution of upvotes and downvotes.\n",
    "- Investigate trends related to the timing of post creation.\n",
    "- Analyze the prevalence of specific words or hashtags within post titles or content.\n",
    "- Unearth relationships between variables, such as the interplay between the number of comments and upvotes.\n",
    "\n",
    "For effective visualization of your findings, you can leverage prominent Python libraries such as Matplotlib, Seaborn, or NetworkX, depending on the precise goals of your analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('all')\n",
    "total_posts_to_retrieve = 10000\n",
    "time_filter = 'year'\n",
    "batch_size = 250\n",
    "all_posts = []\n",
    "retrieved_posts = 0\n",
    "current_batch = 0\n",
    "\n",
    "while retrieved_posts < total_posts_to_retrieve:\n",
    "    remaining_posts = total_posts_to_retrieve - retrieved_posts\n",
    "    posts_to_retrieve = min(remaining_posts, batch_size)\n",
    "    ukraine_posts = list(subreddit.search('#ukrainewar', limit=posts_to_retrieve, time_filter=time_filter, sort='top', params={'before': f'{current_batch}y'}))\n",
    "\n",
    "    if not ukraine_posts:\n",
    "        break\n",
    "\n",
    "    all_posts.extend(ukraine_posts)\n",
    "    retrieved_posts += len(ukraine_posts)\n",
    "    current_batch += 1\n",
    "\n",
    "df = pd.DataFrame(vars(post) for post in all_posts)\n",
    "df = df[['subreddit','selftext','author_fullname','title','upvote_ratio','ups','created','created_utc','num_comments','author','id']]\n",
    "df.to_csv('ukrainewar_full.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NonCredibleDefense</td>\n",
       "      <td></td>\n",
       "      <td>t2_4dfoopu9</td>\n",
       "      <td>Damn...we blinked and missed the T-34 stage of...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>10398</td>\n",
       "      <td>1.666899e+09</td>\n",
       "      <td>1.666899e+09</td>\n",
       "      <td>738</td>\n",
       "      <td>doooompatrol</td>\n",
       "      <td>yf13ra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukraine</td>\n",
       "      <td></td>\n",
       "      <td>t2_3so708z2</td>\n",
       "      <td>Finnishüá´üáÆ volunteer sends greetings home from ...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2100</td>\n",
       "      <td>1.680237e+09</td>\n",
       "      <td>1.680237e+09</td>\n",
       "      <td>57</td>\n",
       "      <td>Vivarevo</td>\n",
       "      <td>127a6ew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UkrainianConflict</td>\n",
       "      <td></td>\n",
       "      <td>t2_p5qo3hyf</td>\n",
       "      <td>Massive #HIMARS strikes 60km from the front in...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>834</td>\n",
       "      <td>1.668659e+09</td>\n",
       "      <td>1.668659e+09</td>\n",
       "      <td>41</td>\n",
       "      <td>Orcasystems99</td>\n",
       "      <td>yxg7ub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>europe</td>\n",
       "      <td>\\nThis megathread is meant for discussion of t...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread LIII</td>\n",
       "      <td>0.95</td>\n",
       "      <td>578</td>\n",
       "      <td>1.680552e+09</td>\n",
       "      <td>1.680552e+09</td>\n",
       "      <td>8232</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>12aw2q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>europe</td>\n",
       "      <td>This megathread is meant for discussion of the...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread L</td>\n",
       "      <td>0.96</td>\n",
       "      <td>424</td>\n",
       "      <td>1.673995e+09</td>\n",
       "      <td>1.673995e+09</td>\n",
       "      <td>9524</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>UkrainWarMonitor</td>\n",
       "      <td></td>\n",
       "      <td>t2_7q9ji3dc</td>\n",
       "      <td>Gunfire erupted after Russian soldiers didn't ...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>3</td>\n",
       "      <td>1.667099e+09</td>\n",
       "      <td>1.667099e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>mazstocks</td>\n",
       "      <td>yh2ynz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>caps</td>\n",
       "      <td>Keep on losing and try to move up the draft lo...</td>\n",
       "      <td>t2_701pa</td>\n",
       "      <td>Lucky Guess - Game 80: vs NYI - Blunder for Be...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3</td>\n",
       "      <td>1.681129e+09</td>\n",
       "      <td>1.681129e+09</td>\n",
       "      <td>29</td>\n",
       "      <td>mdkss12</td>\n",
       "      <td>12hh318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>UkrainWarMonitor</td>\n",
       "      <td></td>\n",
       "      <td>t2_7q9ji3dc</td>\n",
       "      <td>üî• Drone armed with a powerful bomb destroys Ru...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.678322e+09</td>\n",
       "      <td>1.678322e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>mazstocks</td>\n",
       "      <td>11mdcf7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>u_liberty_ukraine</td>\n",
       "      <td>\\n\\nLiberty Ukraine in Action! Thank you for ...</td>\n",
       "      <td>t2_a2tmiv6l</td>\n",
       "      <td>Liberty Ukraine in Action!</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.677530e+09</td>\n",
       "      <td>1.677530e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>liberty_ukraine</td>\n",
       "      <td>11dlwx9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>caps</td>\n",
       "      <td>Beat the best team in the league, then lose to...</td>\n",
       "      <td>t2_701pa</td>\n",
       "      <td>Lucky Guess - Game 56: vs CAR</td>\n",
       "      <td>0.67</td>\n",
       "      <td>4</td>\n",
       "      <td>1.676384e+09</td>\n",
       "      <td>1.676384e+09</td>\n",
       "      <td>38</td>\n",
       "      <td>mdkss12</td>\n",
       "      <td>11269ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit                                           selftext  \\\n",
       "0     NonCredibleDefense                                                      \n",
       "1                ukraine                                                      \n",
       "2      UkrainianConflict                                                      \n",
       "3                 europe  \\nThis megathread is meant for discussion of t...   \n",
       "4                 europe  This megathread is meant for discussion of the...   \n",
       "...                  ...                                                ...   \n",
       "9995    UkrainWarMonitor                                                      \n",
       "9996                caps  Keep on losing and try to move up the draft lo...   \n",
       "9997    UkrainWarMonitor                                                      \n",
       "9998   u_liberty_ukraine   \\n\\nLiberty Ukraine in Action! Thank you for ...   \n",
       "9999                caps  Beat the best team in the league, then lose to...   \n",
       "\n",
       "     author_fullname                                              title  \\\n",
       "0        t2_4dfoopu9  Damn...we blinked and missed the T-34 stage of...   \n",
       "1        t2_3so708z2  Finnishüá´üáÆ volunteer sends greetings home from ...   \n",
       "2        t2_p5qo3hyf  Massive #HIMARS strikes 60km from the front in...   \n",
       "3           t2_ojkhp                     War in Ukraine Megathread LIII   \n",
       "4           t2_ojkhp                        War in Ukraine Megathread L   \n",
       "...              ...                                                ...   \n",
       "9995     t2_7q9ji3dc  Gunfire erupted after Russian soldiers didn't ...   \n",
       "9996        t2_701pa  Lucky Guess - Game 80: vs NYI - Blunder for Be...   \n",
       "9997     t2_7q9ji3dc  üî• Drone armed with a powerful bomb destroys Ru...   \n",
       "9998     t2_a2tmiv6l                         Liberty Ukraine in Action!   \n",
       "9999        t2_701pa                      Lucky Guess - Game 56: vs CAR   \n",
       "\n",
       "      upvote_ratio    ups       created   created_utc  num_comments  \\\n",
       "0             0.99  10398  1.666899e+09  1.666899e+09           738   \n",
       "1             1.00   2100  1.680237e+09  1.680237e+09            57   \n",
       "2             0.99    834  1.668659e+09  1.668659e+09            41   \n",
       "3             0.95    578  1.680552e+09  1.680552e+09          8232   \n",
       "4             0.96    424  1.673995e+09  1.673995e+09          9524   \n",
       "...            ...    ...           ...           ...           ...   \n",
       "9995          0.72      3  1.667099e+09  1.667099e+09             0   \n",
       "9996          0.64      3  1.681129e+09  1.681129e+09            29   \n",
       "9997          1.00      4  1.678322e+09  1.678322e+09             0   \n",
       "9998          1.00      4  1.677530e+09  1.677530e+09             0   \n",
       "9999          0.67      4  1.676384e+09  1.676384e+09            38   \n",
       "\n",
       "                  author       id  \n",
       "0           doooompatrol   yf13ra  \n",
       "1               Vivarevo  127a6ew  \n",
       "2          Orcasystems99   yxg7ub  \n",
       "3     ModeratorsOfEurope  12aw2q2  \n",
       "4     ModeratorsOfEurope  10eps9y  \n",
       "...                  ...      ...  \n",
       "9995           mazstocks   yh2ynz  \n",
       "9996             mdkss12  12hh318  \n",
       "9997           mazstocks  11mdcf7  \n",
       "9998     liberty_ukraine  11dlwx9  \n",
       "9999             mdkss12  11269ca  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Data Processing and Deduplication\n",
    "\n",
    "After retrieving a substantial number of posts from Reddit, the next crucial step involves processing and deduplicating the data. This process ensures that your dataset remains structured and free from redundant entries. The code for this step is summarized as follows:\n",
    "\n",
    "1. **DataFrame Copy**: The DataFrame `unique_posts` is created as a copy of the original DataFrame (`df`). This copy is used for subsequent operations to preserve the integrity of the original data.\n",
    "\n",
    "2. **Sorting by Number of Comments**: The `unique_posts` DataFrame is sorted in descending order based on the number of comments each post has received. This operation allows you to prioritize posts with the highest engagement.\n",
    "\n",
    "3. **Deduplication**: To eliminate duplicate posts, the `drop_duplicates()` method is applied to the `unique_posts` DataFrame. It ensures that only one instance of each unique post, identified by its unique identifier (`id`), remains in the dataset. All additional duplicate entries are removed, resulting in a dataset with no redundant posts.\n",
    "\n",
    "The resulting `unique_posts` DataFrame is now optimized for further analysis, containing distinct posts ordered by their level of engagement (number of comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>europe</td>\n",
       "      <td>This megathread is meant for discussion of the...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread L</td>\n",
       "      <td>0.96</td>\n",
       "      <td>426</td>\n",
       "      <td>1.673995e+09</td>\n",
       "      <td>1.673995e+09</td>\n",
       "      <td>9524</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>europe</td>\n",
       "      <td>This megathread is meant for discussion of the...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread XLIX</td>\n",
       "      <td>0.97</td>\n",
       "      <td>342</td>\n",
       "      <td>1.670887e+09</td>\n",
       "      <td>1.670887e+09</td>\n",
       "      <td>8921</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>zkf1p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>europe</td>\n",
       "      <td>This megathread is meant for discussion of the...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread XLVII</td>\n",
       "      <td>0.94</td>\n",
       "      <td>269</td>\n",
       "      <td>1.667165e+09</td>\n",
       "      <td>1.667165e+09</td>\n",
       "      <td>8439</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>europe</td>\n",
       "      <td>This is a special megathread. **One year ago, ...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread LII</td>\n",
       "      <td>0.97</td>\n",
       "      <td>406</td>\n",
       "      <td>1.677156e+09</td>\n",
       "      <td>1.677156e+09</td>\n",
       "      <td>8276</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>119wltg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>europe</td>\n",
       "      <td>\\nThis megathread is meant for discussion of t...</td>\n",
       "      <td>t2_ojkhp</td>\n",
       "      <td>War in Ukraine Megathread LIII</td>\n",
       "      <td>0.95</td>\n",
       "      <td>583</td>\n",
       "      <td>1.680552e+09</td>\n",
       "      <td>1.680552e+09</td>\n",
       "      <td>8232</td>\n",
       "      <td>ModeratorsOfEurope</td>\n",
       "      <td>12aw2q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>UkrainWarMonitor</td>\n",
       "      <td></td>\n",
       "      <td>t2_7q9ji3dc</td>\n",
       "      <td>Ukraine war footage: Intense combat take over ...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>11</td>\n",
       "      <td>1.678719e+09</td>\n",
       "      <td>1.678719e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>mazstocks</td>\n",
       "      <td>11qbqqt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>UkraineConflict</td>\n",
       "      <td></td>\n",
       "      <td>t2_9dwibqf3</td>\n",
       "      <td>Gen. Ben Hodges Ukrainians üá∫üá¶ are shockingly g...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15</td>\n",
       "      <td>1.678891e+09</td>\n",
       "      <td>1.678891e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>Aminokef</td>\n",
       "      <td>11ryhx9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>UkrainWarMonitor</td>\n",
       "      <td></td>\n",
       "      <td>t2_7q9ji3dc</td>\n",
       "      <td>Close combat footage from Ukraine - battle wit...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>9</td>\n",
       "      <td>1.678209e+09</td>\n",
       "      <td>1.678209e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>mazstocks</td>\n",
       "      <td>11l4y9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>Wallstreetsilver</td>\n",
       "      <td></td>\n",
       "      <td>t2_79blzkek</td>\n",
       "      <td>#wef#bank#russia#ukraine#ukrainewar#petrol#bar...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>29</td>\n",
       "      <td>1.670960e+09</td>\n",
       "      <td>1.670960e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok_Entertainer_6860</td>\n",
       "      <td>zl4s68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>2easterneuropean4u</td>\n",
       "      <td></td>\n",
       "      <td>t2_ryfa2wz6</td>\n",
       "      <td>Damn...we blinked and missed the T-34 stage of...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>36</td>\n",
       "      <td>1.666906e+09</td>\n",
       "      <td>1.666906e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>Korolenko_</td>\n",
       "      <td>yf3z2k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit                                           selftext  \\\n",
       "3860              europe  This megathread is meant for discussion of the...   \n",
       "3141              europe  This megathread is meant for discussion of the...   \n",
       "2661              europe  This megathread is meant for discussion of the...   \n",
       "246               europe  This is a special megathread. **One year ago, ...   \n",
       "1208              europe  \\nThis megathread is meant for discussion of t...   \n",
       "...                  ...                                                ...   \n",
       "7027    UkrainWarMonitor                                                      \n",
       "7024     UkraineConflict                                                      \n",
       "7042    UkrainWarMonitor                                                      \n",
       "7016    Wallstreetsilver                                                      \n",
       "7014  2easterneuropean4u                                                      \n",
       "\n",
       "     author_fullname                                              title  \\\n",
       "3860        t2_ojkhp                        War in Ukraine Megathread L   \n",
       "3141        t2_ojkhp                     War in Ukraine Megathread XLIX   \n",
       "2661        t2_ojkhp                    War in Ukraine Megathread XLVII   \n",
       "246         t2_ojkhp                      War in Ukraine Megathread LII   \n",
       "1208        t2_ojkhp                     War in Ukraine Megathread LIII   \n",
       "...              ...                                                ...   \n",
       "7027     t2_7q9ji3dc  Ukraine war footage: Intense combat take over ...   \n",
       "7024     t2_9dwibqf3  Gen. Ben Hodges Ukrainians üá∫üá¶ are shockingly g...   \n",
       "7042     t2_7q9ji3dc  Close combat footage from Ukraine - battle wit...   \n",
       "7016     t2_79blzkek  #wef#bank#russia#ukraine#ukrainewar#petrol#bar...   \n",
       "7014     t2_ryfa2wz6  Damn...we blinked and missed the T-34 stage of...   \n",
       "\n",
       "      upvote_ratio  ups       created   created_utc  num_comments  \\\n",
       "3860          0.96  426  1.673995e+09  1.673995e+09          9524   \n",
       "3141          0.97  342  1.670887e+09  1.670887e+09          8921   \n",
       "2661          0.94  269  1.667165e+09  1.667165e+09          8439   \n",
       "246           0.97  406  1.677156e+09  1.677156e+09          8276   \n",
       "1208          0.95  583  1.680552e+09  1.680552e+09          8232   \n",
       "...            ...  ...           ...           ...           ...   \n",
       "7027          0.88   11  1.678719e+09  1.678719e+09             0   \n",
       "7024          1.00   15  1.678891e+09  1.678891e+09             0   \n",
       "7042          0.86    9  1.678209e+09  1.678209e+09             0   \n",
       "7016          0.84   29  1.670960e+09  1.670960e+09             0   \n",
       "7014          0.97   36  1.666906e+09  1.666906e+09             0   \n",
       "\n",
       "                   author       id  \n",
       "3860   ModeratorsOfEurope  10eps9y  \n",
       "3141   ModeratorsOfEurope   zkf1p4  \n",
       "2661   ModeratorsOfEurope   yhqh49  \n",
       "246    ModeratorsOfEurope  119wltg  \n",
       "1208   ModeratorsOfEurope  12aw2q2  \n",
       "...                   ...      ...  \n",
       "7027            mazstocks  11qbqqt  \n",
       "7024             Aminokef  11ryhx9  \n",
       "7042            mazstocks  11l4y9a  \n",
       "7016  Ok_Entertainer_6860   zl4s68  \n",
       "7014           Korolenko_   yf3z2k  \n",
       "\n",
       "[241 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_posts = df.copy()\n",
    "unique_posts.sort_values(by='num_comments',ascending=False,inplace=True)\n",
    "unique_posts.drop_duplicates(subset='id',inplace=True)\n",
    "unique_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Extracting Comments for Reddit Posts\n",
    "\n",
    "This code snippet outlines the process of extracting comments related to Reddit posts. It utilizes the Python Reddit API Wrapper (PRAW) library and the information obtained from the previous steps. The explanation of the code is as follows:\n",
    "\n",
    "1. **Empty List Creation**: An empty list named `comments_list` is initialized. This list will be used to store extracted comment data.\n",
    "\n",
    "2. **Comment Extraction Function**: The code defines a function called `extract_comments(post_id)`. This function takes a post's unique identifier (`post_id`) as a parameter.\n",
    "\n",
    "3. **Reddit Submission Retrieval**: Within the function, a Reddit post submission is retrieved using the provided `post_id`. This submission object represents the specific post on Reddit.\n",
    "\n",
    "4. **Replacing More Comments**: To ensure all comments are retrieved, the code uses `submission.comments.replace_more(limit=None)` to replace more comments. This is necessary because Reddit employs a \"load more comments\" mechanism that can hide some comments from direct retrieval. This function effectively retrieves all comments.\n",
    "\n",
    "5. **Comment Data Extraction**: The code then iterates through the comments using a list comprehension. For each comment, it extracts three pieces of information: the `comment_id` (unique comment identifier), `comment_body` (the text content of the comment), and `post_id` (the identifier of the post to which the comment is related). These details are stored in a dictionary.\n",
    "\n",
    "6. **Appending to `comments_list`**: The extracted comment data is appended to the `comments_list` as a dictionary. This process ensures that comments from multiple posts are gathered in a single list for later analysis.\n",
    "\n",
    "By using this code, you can collect comments related to Reddit posts, making them available for further processing and analysis. Each comment is associated with its parent post, which can be valuable for understanding user engagement and sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = []\n",
    "\n",
    "def extract_comments(post_id):\n",
    "    submission = reddit.submission(id=post_id)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comments_list.extend([{'comment_id': comment.id,'comment_body': comment.body,'post_id': post_id,} for comment in submission.comments.list()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Extracting Comments from Reddit Posts\n",
    "\n",
    "In this code block, we see the process of extracting comments from Reddit posts. The code is designed to handle potential rate limiting by Reddit's API while collecting the comments. Here's a step-by-step explanation of the code:\n",
    "\n",
    "1. **Iteration Through Posts**: The code starts by initializing a variable `i` to zero. It then iterates through the unique post identifiers stored in the DataFrame `unique_posts` using the 'id' column.\n",
    "\n",
    "2. **Rate Limit Handling**: Within the loop, the code includes a rate limit handling mechanism. The code checks if `i` (the rate limit counter) is less than 4, indicating that we will attempt to collect comments from up to four posts.\n",
    "\n",
    "3. **Try-Except Block**: Inside the loop, there's a try-except block. It attempts to extract comments from a Reddit post using the `extract_comments` function. If there's an exception (such as reaching the rate limit), the code proceeds to the except block.\n",
    "\n",
    "4. **Rate Limit Exceeded Handling**: In the except block, `i` is incremented by 1, indicating that an attempt to collect comments has been made. The code also prints an informative message indicating that the rate limit was exceeded and that it's waiting for a moment before retrying. A delay of 7 seconds (achieved using `time.sleep(7)`) is introduced to prevent overwhelming the Reddit API with requests.\n",
    "\n",
    "5. **Continuing the Loop**: After handling the exception, the code continues the loop to try collecting comments from the next post. This process repeats up to four times (controlled by the rate limit handling).\n",
    "\n",
    "6. **Creating a DataFrame**: After the loop completes, a DataFrame named `comments_df` is created. This DataFrame is built from the `comments_list`, which accumulates comments from multiple posts. Each row in the DataFrame represents a comment, including the `comment_id`, `comment_body`, and `post_id`.\n",
    "\n",
    "7. **Saving to CSV**: Finally, the DataFrame is saved to a CSV file named 'comments_data.csv' without an index column.\n",
    "\n",
    "By using this code, you can systematically extract comments from multiple Reddit posts while efficiently handling rate limits, making it suitable for data collection tasks that involve Reddit's API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Rate limit exceeded. Waiting for a moment. Error: received 429 HTTP response\n",
      "1\n",
      "1\n",
      "Rate limit exceeded. Waiting for a moment. Error: received 429 HTTP response\n",
      "2\n",
      "Rate limit exceeded. Waiting for a moment. Error: received 429 HTTP response\n",
      "3\n",
      "Rate limit exceeded. Waiting for a moment. Error: received 429 HTTP response\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for post_id in unique_posts['id']:\n",
    "    print(i)\n",
    "\n",
    "    if i < 4: \n",
    "        try:\n",
    "            extract_comments(post_id)\n",
    "        except Exception as error:\n",
    "            i += 1\n",
    "            print(f\"Rate limit exceeded. Waiting for a moment. Error: {error}\")\n",
    "            time.sleep(7)\n",
    "            continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "comments_df = pd.DataFrame(comments_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "comments_df.to_csv('comments_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>j4spbvz</td>\n",
       "      <td>Back in December, we asked for some feedback o...</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j5t53d7</td>\n",
       "      <td>It is funny to read the pro-Russian far-right ...</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>j509t84</td>\n",
       "      <td>&gt; [Denmark donates artillery to Ukraine](https...</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j60rk9u</td>\n",
       "      <td>[Auschwitz museum: Russia not invited to event...</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j4sbv6e</td>\n",
       "      <td>...hoping the L will be the one Russia takes</td>\n",
       "      <td>10eps9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>iuwtttw</td>\n",
       "      <td>You moved the definition of your own words to ...</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>iuwv2om</td>\n",
       "      <td>No, you misinterpreted my definition to mean s...</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>iuwvosc</td>\n",
       "      <td>Okay, you're clearly doing this intentionally.</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>iuwwidt</td>\n",
       "      <td>I think you do.</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>iuwwlz0</td>\n",
       "      <td>ü§¶‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>yhqh49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18183 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                       comment_body  post_id\n",
       "0        j4spbvz  Back in December, we asked for some feedback o...  10eps9y\n",
       "1        j5t53d7  It is funny to read the pro-Russian far-right ...  10eps9y\n",
       "2        j509t84  > [Denmark donates artillery to Ukraine](https...  10eps9y\n",
       "3        j60rk9u  [Auschwitz museum: Russia not invited to event...  10eps9y\n",
       "4        j4sbv6e       ...hoping the L will be the one Russia takes  10eps9y\n",
       "...          ...                                                ...      ...\n",
       "18178    iuwtttw  You moved the definition of your own words to ...   yhqh49\n",
       "18179    iuwv2om  No, you misinterpreted my definition to mean s...   yhqh49\n",
       "18180    iuwvosc     Okay, you're clearly doing this intentionally.   yhqh49\n",
       "18181    iuwwidt                                    I think you do.   yhqh49\n",
       "18182    iuwwlz0                                               ü§¶‚Äç‚ôÇÔ∏è   yhqh49\n",
       "\n",
       "[18183 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
